# ðŸ”¥ TinyShakespeare Transformer Language Model

A character-level Transformer-based language model implemented in PyTorch, trained on the [Tiny Shakespeare](https://huggingface.co/datasets/tiny_shakespeare) dataset. This project walks through the creation of a GPT-style architecture from scratch, showcasing the mechanics of multi-head attention, positional embeddings, and transformer blocks.

---

## ðŸš€ Demo Output

```txt
HERMIONE:
You morror's land; though I am sure to it,
Who didst my shall confine and the valt: so if ye
As, what all thy libert-meanting importation
My valour's obcation as fault: who's given on,
Blessay yet dons a free; the women tumble nott
you. o'erwelth the burn towards. Of her goves good
will a temperat'st man too well-eatens gold
their summers'?

COMINIUS:
With you not; sir,
```

---

## ðŸ§  Key Features

- ðŸ§© Character-level tokenization
- ðŸ”— Transformer-based architecture with self-attention
- ðŸ“¦ Uses Hugging Face's `datasets` to load Tiny Shakespeare
- ðŸ“ˆ Evaluation using loss and perplexity metrics

---

## ðŸ“‚ Project Structure

```
.
â”œâ”€â”€ transformer_shakespeare.py       # Main model + training script
â”œâ”€â”€ Tiny_shakespeare_train.txt       # Processed training data
â”œâ”€â”€ Tiny_shakespeare_validation.txt  # Processed validation data
â”œâ”€â”€ README.md                        # This file
```

---

## ðŸ§ª Model Architecture

- **Embedding Layer**: Token + Positional embeddings
- **Transformer Blocks**: 4 layers, 4 attention heads each
- **Feedforward Layers**: ReLU activated linear layers
- **Loss Function**: Cross-Entropy
- **Generation**: Greedy + Sampling

---

## ðŸ› ï¸ Installation & Setup

> ðŸ“Œ Requires Python 3.8+ and PyTorch 1.12+

### 1. Clone the Repo

```bash
git clone https://github.com/yourusername/tiny-shakespeare-transformer.git
cd tiny-shakespeare-transformer
```

### 2. Create Environment & Install Dependencies

```bash
pip install torch datasets
```

---

## ðŸ“š Usage

### ðŸƒâ€â™‚ï¸ Train the Model

```bash
python transformer_shakespeare.py
```

During training, it will periodically output:

```
step 0: train loss 3.9562, val loss 3.8764
step 100: train loss 2.7514, val loss 2.8213
...
```

### âœï¸ Generate Shakespearean Text

After training, the script will generate 2000 characters of new text based on the trained model:

```txt
ROMEO:
O, speak again, bright angel! for thou art
As glorious to this night...
```

---

## âš™ï¸ Hyperparameters

| Parameter       | Value    |
|----------------|----------|
| `block_size`   | 256      |
| `batch_size`   | 64       |
| `n_embd`       | 128      |
| `n_head`       | 4        |
| `n_layer`      | 4        |
| `dropout`      | 0.0      |
| `learning_rate`| 1e-3     |
| `max_iters`    | 5000     |

Feel free to tweak these in the script to experiment with model size and performance.

---

## ðŸ“ˆ Evaluation

After training:

```txt
Train Loss: 1.2239, Perplexity: 3.40
Val Loss:   1.4894, Perplexity: 4.43
```

> âœ… Lower perplexity = better generative performance.

---

## ðŸ“Ž References

- [Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)
- [Karpathy's nanoGPT](https://github.com/karpathy/nanoGPT)
- [Hugging Face Datasets](https://huggingface.co/datasets/tiny_shakespeare)

---

